{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8QYGawV3o3T",
    "outputId": "866d989f-f454-4657-e23e-0c97e8d0955c"
   },
   "outputs": [],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WMCWVTVV4cGN",
    "outputId": "aaea75a5-2156-4910-9d0a-2fcd9ec3f24e"
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# Fill these with your Reddit app credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id='hsEaEyvNyz40hQGCx5LkGQ',\n",
    "    client_secret='m2Wg9wPGCk_M-UMzgFCEvYfCPOvzSg',\n",
    "    user_agent='networkscience'\n",
    ")\n",
    "\n",
    "# üîç Keywords to search for\n",
    "keywords = [\"climate\", \"melting\", \"ice\", \"pollution\", \"co2\", \"oil\", \"gasoline\", \"electric\", \"energy\"]\n",
    "\n",
    "# üåê Subreddits to search in\n",
    "subreddits = [\"climate\", \"climatechange\", \"environment\", \"science\", \"sustainability\", \"energy\", \"renewableenergy\"]\n",
    "\n",
    "def collect_comments(keywords, subreddits, posts_per_combo=20, comments_per_post=20, max_comments=15000):\n",
    "    data = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"\\nüîé Searching for keyword: '{keyword}'\")\n",
    "        for subreddit_name in subreddits:\n",
    "            print(f\"  ‚Üí in subreddit: r/{subreddit_name}\")\n",
    "            try:\n",
    "                subreddit = reddit.subreddit(subreddit_name)\n",
    "                for submission in subreddit.search(keyword, sort='new', limit=posts_per_combo):\n",
    "                    submission.comments.replace_more(limit=0)\n",
    "                    for comment in submission.comments[:comments_per_post]:\n",
    "                        data.append({\n",
    "                            \"keyword\": keyword,\n",
    "                            \"subreddit\": subreddit_name,\n",
    "                            \"submission_title\": submission.title,\n",
    "                            \"comment_body\": comment.body,\n",
    "                            \"author\": str(comment.author),\n",
    "                            \"created_utc\": comment.created_utc,\n",
    "                        })\n",
    "\n",
    "                        # üõë Stop once we've reached the target\n",
    "                        if len(data) >= max_comments:\n",
    "                            print(f\"\\n‚úÖ Reached {max_comments} comments. Stopping collection.\")\n",
    "                            return pd.DataFrame(data)\n",
    "\n",
    "                sleep(1)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error in r/{subreddit_name}: {e}\")\n",
    "                sleep(2)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = collect_comments(keywords, subreddits, posts_per_combo=20, comments_per_post=20, max_comments=15000)\n",
    "\n",
    "df.to_csv(\"reddit_climate_comments_full.csv\", index=False)\n",
    "print(\"\\n‚úÖ Dataset saved to: reddit_climate_comments_full.csv\")\n",
    "print(f\"üìä Total collected comments: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3PbWvJ3N89Lp",
    "outputId": "49aff02a-bcac-4279-e454-92553d285840"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_climate_comments_full.csv\")\n",
    "\n",
    "print(\"üî¢ Shape of dataset:\", df.shape)\n",
    "\n",
    "print(\"üìã Columns:\", df.columns.tolist())\n",
    "\n",
    "print(\"\\nüîç Sample rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S62I8WUI7bFH",
    "outputId": "b17edcbf-7bcc-438c-edf3-da01ea4dc1d9"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  # workaround for buggy environments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPINL2mD-b23",
    "outputId": "27bf1fc6-d726-495b-e362-36f9e5e1851b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load raw dataset\n",
    "df = pd.read_csv(\"reddit_climate_comments_full.csv\")\n",
    "\n",
    "# 1. Remove duplicate comments\n",
    "df = df.drop_duplicates(subset=\"comment_body\")\n",
    "\n",
    "# 2. Drop empty or very short comments (< 10 characters)\n",
    "df = df[df[\"comment_body\"].astype(str).str.strip().str.len() > 10]\n",
    "\n",
    "# 3. Filter irrelevant content (deleted, removed, AutoModerator)\n",
    "df = df[~df[\"comment_body\"].str.contains(\"removed|deleted\", case=False, na=False)]\n",
    "df = df[df[\"author\"].str.lower() != \"automoderator\"]\n",
    "\n",
    "# 4. Clean text: lowercase, remove links, extra spaces\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text)        # remove punctuation/numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()    # remove extra whitespace\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"comment_body\"].apply(clean_text)\n",
    "\n",
    "# 5. Tokenize the clean text (for future NLP usage)\n",
    "df[\"tokens\"] = df[\"clean_text\"].apply(word_tokenize)\n",
    "\n",
    "# 6. Drop rows with less than 3 tokens (optional quality control)\n",
    "df = df[df[\"tokens\"].apply(len) >= 3]\n",
    "\n",
    "# 7. Reset index and show result\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Save cleaned dataset\n",
    "df.to_csv(\"reddit_climate_cleaned.csv\", index=False)\n",
    "print(\"‚úÖ Cleaned dataset saved as 'reddit_climate_cleaned.csv'\")\n",
    "print(\"‚úÖ Final shape:\", df.shape)\n",
    "print(df[[\"comment_body\", \"clean_text\", \"tokens\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waJTGGgv_MCu"
   },
   "source": [
    "community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5db222caef3a41c88fef8d3d665cbdcc",
      "d5eb972362344298bf3a0b114793529c",
      "87f57bd6bbc44621b47d5956a4cf380c",
      "24d1678b5f234ff29c70e762b0143de7",
      "8e5def11cc8540e1893fddc95bc2c213",
      "0d09bebbcbbe41a5b3d15f37e411f94b",
      "aa0bb54bde1b48279caa9ccfaa748d51",
      "3b3a3f85d9334290b63758f897dda142",
      "90e57d6b22a6437e819bfb5e994460cf",
      "dedd3b789ff44b8cacdbe0a2b5b98f89",
      "11e854566ac54e97b448c81fa3092298",
      "6afed67713bb4d81804a1664881890b8",
      "15b527b1d9b241d2805d63dcf9c6f7ae",
      "0902c4e1ca3d4fa4b2457aaf4261f02a",
      "1b284b148f2745a8aad387c4320470be",
      "8ff4563302314bab8c03ed415ca5c804",
      "7bd816ff3cb84d34a64aa2e95ed1fab8",
      "9aebf454bc944d4dab2285c1d66dee35",
      "c614826700e341b88744fcb3ebcc8deb",
      "613239d80d5543f794ffc1104ec03e06",
      "33a9c9090d7f409c8b5d2c075bd8f676",
      "c9e73dd0c214449a8663b8930d978056",
      "d82305c0f38845e2b24d118bf9a624d0",
      "60e6f6337b5b4768893a5222cf1a9862",
      "cd9f22b6da4f40688bd7014117f315dd",
      "6a7426fd892c42f3b06e4ff2a0e9e239",
      "5b19b9cf42334deeb796b3f8e813b596",
      "376f23b452914428ad2a34a437445c04",
      "10c2431e11c046558f628214e1ff8130",
      "336c5457cd8c4bd88fb6a5416c103bd8",
      "e71a8cb82d3a45feb96759ed85bae27e",
      "359bf4da4f674b4aa9dd4725e72fffbd",
      "a432b6c3c67a4242bc44a9880097f89b",
      "549129c62d3344bdafc3db7ff3b649df",
      "822c4e6457ba470481b3bbdba506f4c0",
      "f8858bd761d344048e969ad015ff1004",
      "54e157c6e666440bb2e43c45b35c4932",
      "1f1b3d2bf6b9431c881cbd8bdd796e1c",
      "472d7e34880948b59c4dc9d09853522f",
      "af1fc18308c04382861da826f50e7e34",
      "71d259230913448cbe85e9d64d4834ba",
      "82202b9733d142e898918de0f6e08a1a",
      "f65cb0bae690418cbcc0a83cb4530be0",
      "ce282fde62904bfbac6a0f95a9d1e55f",
      "f85c5820c2aa4487ae9a224dfd3bad7a",
      "eabcd2c83d07461394d3d7fabf5041b1",
      "c753869289f34efe9bfa0e16a9878b6d",
      "034588ddac494d1b8ad1c7118151fee8",
      "bbf451aa71ed4847a17003a870cfc91c",
      "7e3d5314815245f1804d52967acf3ccf",
      "e6d2c50f4e034f4f945015ccfb776b4a",
      "09aafa74d9a143e3b4d11d1bb4fd5e74",
      "43bba4c45e7b4b10b3e834aeb7c873eb",
      "94a44c2cf1be447aa9e4faee45c74b66",
      "e7c3bc59d5ff417e9a419ea419a85f6c",
      "1a232e58bd7f46f885191fd9ec504cb5",
      "3b7f3ce41ee54b13a6e56ae1706a1115",
      "c98325957fd443b09217547f5a047ff5",
      "5fbea03e02244334ac4b0a6c5c79a257",
      "5df2f923dc4f47808c4a58c48eb051f3",
      "599c5ba770a7464eb3887b2359a8f76b",
      "3b9d95505b454d46afeb12ddd0007843",
      "71e967ba1ed14ce8a54ee291771bf04b",
      "6bc2a135806047e2b0a322fe78550d98",
      "9b71cf7391dc441b8d92a785a175fec7",
      "0485284406874a58a8d5a4950fc58313",
      "ec858a3f4b834821b84ea497401b2914",
      "594474ab6b3447fa896df4f60f9689e0",
      "103d98598510449abc3a06413c0f87f2",
      "76c70bb4046c4c469fcb3a0198264326",
      "d45d3d2754434ca89c9da6201c96d954",
      "f4aa116cf89342f78f8f617dca73a6a0",
      "deb23d546d394eda942a8f56e06bad54",
      "6221138803a14140a9a5692ec5c8c8bf",
      "016d42efe29048d38c941042350b544b",
      "2f776040993d4bf582cad2a8c78d3fba",
      "827720fc948b4fa3b08dba86f77e5f69",
      "db291d66777b4b8d9a1d71a2fb54e8aa",
      "b983961cf1c742c48d7d2c45016a174b",
      "f1934451f92f41deb8597bd4a4e345ab",
      "e39ba8e4a7e04e7d867493b59d722a0b",
      "5d94f77d4760476aad1a376af247df04",
      "274a67275f234e2e976f60b0b4bb4aa6",
      "811c0a0e9f3d403fb83b20d3707d14be",
      "2b6f318ba1594456b13c80109209ac48",
      "559a87c9d4a549018e525570a0e2fcd6",
      "5ed80c423f404434882441f38296c453",
      "23f2da826a774418930fd222fd8cb1d0",
      "fbbe7f235e674aa2b4ca0f18001ea0fa",
      "5e0f30f36d7b43d2b95495a260df85f3",
      "e623ba34d79946f4a533120d1f7cfc3b",
      "57db34b5864f4577b8ab811998faa78b",
      "36d214e77346466eb4b05053794720a7",
      "1a3c6e61869d4829b6dc7ed381d5956f",
      "4fe2d48332354de99dcaebe7bd5b76c5",
      "bbf570b886824d13b3ace182646423dc",
      "bf723a3631994b21a0c20d5e5eab8614",
      "bb527148286c46a487379d30beea9d62",
      "e99c4006f7ab418c83d2e236c0d3af8d",
      "2ccfbb6acfd84971ba243a40c716d887",
      "c01d280861504b53820a0dc84d8cfaa4",
      "0446d3af4e7c455bb9a1656c9693654d",
      "31a1c7a3537b48f2875d620fb0de9ea5",
      "9b4fc2ea322946599f7e1f4d4f386065",
      "aa376f40a3994752bd00aa5596596eab",
      "dc3fde8ad6af43e5bec9b5fbb63d8527",
      "8a9a9c2d88f44ed792943b8cc1e1ab0b",
      "2851a594fc7f40908d0960da3aef793a",
      "fab0aea3161b46d39d483d992f1b08e5",
      "d7e8c174326c4c5dab5b440e66987963",
      "04f5a350d4fe47549885b0d0d6eed4d3",
      "547724fa484043759f1ed2dde0ed57e3",
      "3f299250b67a43f7b84c007bdd9c44ac",
      "359a35ed906c48b0894156e3e90b4801",
      "9a5eac9586a1421c9f03024279181338",
      "b71afcb0acdf427d9c127f3d68598308",
      "602b37d541264523be457bb97c40133b",
      "29ab1e6509074af4af4dfb7c6575d6c8",
      "81939eccf9664ce2844dc2540df8d645",
      "b69ee76231e741deab9136934381ddfa",
      "03ee23b5b1b24dadb404e2d5866a0d56",
      "acd96eb00bfe4e388dee28c255d52549",
      "f10005cf526d416eba6d724e79a3078d",
      "f6ed0b43d7f441b0a932551cb5bbdda6",
      "d51eee54a1e24b269d0d13e045c8415f",
      "599c08d718b149a89f067abb5e339fbe",
      "84382639a9f646fbaed1650b3ff9acb3",
      "61eb5f5c427f4970868360ad162482e5",
      "e8db31b9e31c4878b8f6ec7a7b3ef407",
      "045a7a32b38b425ab91b3151a4c8461c",
      "cdf80727ef754e01b4e0399e8b3fcf50",
      "cb675a6549914ecc81fc324d650784d4"
     ]
    },
    "id": "mfDAyBNV_MNx",
    "outputId": "b0b51b9d-e725-4cce-f819-f5bd3f00c8c3"
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers networkx community matplotlib seaborn -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import community as community_louvain\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "\n",
    "# üîπ Step 1: Get embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# üîπ Step 2: Compute cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# üîπ Step 3: Build similarity graph\n",
    "G = nx.Graph()\n",
    "threshold = 0.6  # adjust for density\n",
    "\n",
    "for i in tqdm(range(len(texts))):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        if sim > threshold:\n",
    "            G.add_edge(i, j, weight=sim)\n",
    "\n",
    "print(f\"‚úÖ Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# üîπ Step 4: Louvain community detection\n",
    "partition = community_louvain.best_partition(G)\n",
    "df[\"louvain_community\"] = df.index.map(partition)\n",
    "\n",
    "# üîπ Save updated dataset\n",
    "df.to_csv(\"reddit_with_louvain.csv\", index=False)\n",
    "print(\"‚úÖ Saved with community labels\")\n",
    "\n",
    "# üî∏ Step 5: Plot community size\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=\"louvain_community\", data=df, order=df[\"louvain_community\"].value_counts().index)\n",
    "plt.title(\"Community Size Distribution\")\n",
    "plt.xlabel(\"Community ID\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üî∏ Step 6: Network Graph Visualization (sample 300 nodes)\n",
    "sample_nodes = list(G.nodes)[:300]\n",
    "subG = G.subgraph(sample_nodes)\n",
    "pos = nx.spring_layout(subG, seed=42)\n",
    "colors = [partition[n] for n in subG.nodes]\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw_networkx_nodes(subG, pos, node_size=40, node_color=colors, cmap='tab20')\n",
    "nx.draw_networkx_edges(subG, pos, alpha=0.2)\n",
    "plt.title(\"Reddit Comments Graph with Louvain Communities\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# üî∏ Step 7: Word Cloud for each top community\n",
    "top_communities = df[\"louvain_community\"].value_counts().head(5).index\n",
    "\n",
    "for com in top_communities:\n",
    "    text = \" \".join(df[df[\"louvain_community\"] == com][\"clean_text\"])\n",
    "    wc = WordCloud(width=800, height=300, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for Community {com}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqJiH5rmC7Dl"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c79831b19c884d6fa11402cfb5e7881a",
      "092a995cb5014feda653ff0894cc1216",
      "344d6dc7522d4e89a82e2bf3ddf267aa",
      "f17920bb3ae441829b55446d91fb78d2",
      "71ca5f11b7464988b8917081ee6c2705",
      "96465b2b6e044884976b6971e572c05d",
      "1841c16c9a374c1089b6a66a3c0af8d5",
      "09fa6475e2fb4f52bc4b8d7919d2aae7",
      "ab7897b76b6944ea8cc0c94331a46d54",
      "353544a8af854b74b27cdc4c13b2b35d",
      "e10ebf2cc88b4b1eab117a639e9684e0"
     ]
    },
    "id": "p1ktDKTdC7QU",
    "outputId": "58150c55-991e-4bcf-b6bb-d391a60b8ee6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "\n",
    "# Load preprocessed and labeled data\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "\n",
    "# Ensure community column is int\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# === 1. COMMUNITY SIZE BAR PLOT ===\n",
    "plt.figure(figsize=(10, 5))\n",
    "order = df[\"louvain_community\"].value_counts().index\n",
    "sns.countplot(data=df, x=\"louvain_community\", order=order, palette=\"tab20\")\n",
    "plt.title(\"üß© Number of Comments per Community\", fontsize=14)\n",
    "plt.xlabel(\"Community ID\", fontsize=12)\n",
    "plt.ylabel(\"Number of Comments\", fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 2. TEXTUAL PREVIEW ===\n",
    "top_communities = df[\"louvain_community\"].value_counts().head(5).index\n",
    "print(\"üìò Sample comments from top communities:\")\n",
    "for com in top_communities:\n",
    "    sample = df[df[\"louvain_community\"] == com][\"clean_text\"].iloc[0]\n",
    "    print(f\"\\nüü¶ Community {com} example:\")\n",
    "    print(\"  \", sample)\n",
    "\n",
    "# === 3. WORD CLOUDS FOR EACH COMMUNITY ===\n",
    "for com in top_communities:\n",
    "    text = \" \".join(df[df[\"louvain_community\"] == com][\"clean_text\"])\n",
    "    wc = WordCloud(width=1000, height=300, background_color=\"white\", max_words=100).generate(text)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"‚òÅÔ∏è Word Cloud ‚Äì Community {com}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === 4. NETWORK GRAPH (Cleaner) ===\n",
    "import networkx as nx\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Reload graph (optional, if not kept)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "threshold = 0.6\n",
    "G = nx.Graph()\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        if sim_matrix[i][j] > threshold:\n",
    "            G.add_edge(i, j, weight=sim_matrix[i][j])\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "\n",
    "# Draw a subgraph (e.g. top 200 nodes)\n",
    "sub_nodes = list(G.nodes)[:200]\n",
    "subG = G.subgraph(sub_nodes)\n",
    "pos = nx.spring_layout(subG, seed=42)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = [partition[n] for n in subG.nodes]\n",
    "nx.draw_networkx_nodes(subG, pos, node_size=40, node_color=colors, cmap=cm.get_cmap('tab20'))\n",
    "nx.draw_networkx_edges(subG, pos, alpha=0.2)\n",
    "plt.title(\"üåê Reddit Comment Similarity Graph (Louvain Coloring)\", fontsize=14)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === 5. DEGREE DISTRIBUTION ===\n",
    "degrees = [val for (node, val) in G.degree()]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(degrees, bins=50, kde=False, color=\"purple\")\n",
    "plt.title(\"üìä Degree Distribution of Reddit Comment Graph\", fontsize=14)\n",
    "plt.xlabel(\"Degree (number of edges)\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqN2I5qWDhtW"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e27355b63b84495f8d9f4ae5038a7578",
      "f8d42e86b38e469d93cbee58e5ec0696",
      "0a31e8556b4441ffb03b17ba0a924ffe",
      "6b17293d2a8a4b0a9ba6df7176badf65",
      "0f0c6d377e9a4bc28577383bcbfa2270",
      "003d0f7f898b4f5d98a8d63fb29a4cae",
      "a7019108ee584b69b2f5256cf1cee9ac",
      "3555b4d6e4d14a0fa62e34784035cc3d",
      "4526810f620a4466a94f6d43c2e64eee",
      "545029af30544843a563b72cfc4cd57c",
      "7e23692fe2934f75acd8d40f118bb29a"
     ]
    },
    "id": "kNz4QZC3Dh27",
    "outputId": "58c989c9-9877-403a-e583-230cbf7c33c7"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn matplotlib seaborn networkx community -q\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import community as community_louvain\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts = df[\"clean_text\"].tolist()\n",
    "\n",
    "# Get embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Build similarity graph\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "threshold = 0.6\n",
    "G = nx.Graph()\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i+1, len(texts)):\n",
    "        if sim_matrix[i][j] > threshold:\n",
    "            G.add_edge(i, j, weight=sim_matrix[i][j])\n",
    "\n",
    "# Louvain clustering\n",
    "partition = community_louvain.best_partition(G)\n",
    "df[\"louvain_community\"] = df.index.map(partition).fillna(-1).astype(int)\n",
    "\n",
    "# Use t-SNE for layout\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "df[\"tsne-1\"] = tsne_results[:, 0]\n",
    "df[\"tsne-2\"] = tsne_results[:, 1]\n",
    "\n",
    "# Plot with Seaborn\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-1\", y=\"tsne-2\",\n",
    "    hue=\"louvain_community\",\n",
    "    palette=\"tab20\",\n",
    "    data=df,\n",
    "    legend=\"full\",\n",
    "    s=25\n",
    ")\n",
    "plt.title(\"üåê t-SNE Visualization of Reddit Comment Communities\", fontsize=16)\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9A60jH3EVO1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839,
     "referenced_widgets": [
      "5f9db5dba0c1498ca7a3a6b33d8de6d9",
      "a395bb7eacbd4a74ac7fb52c649e2b0b",
      "7ca1bb032f1140c6b4686a3547e89d80",
      "9afe561fba0b432b848d903fa59bf178",
      "a767530224a846e79dbe975419e56ce7",
      "3d4570956ff643929d588204a525ea65",
      "d31613ac35a64b928ae84b638e2820b1",
      "4c3ead5b593a4972abf4fa00cdbad7d1",
      "dceea16e94bd4385a0690165949f7ca6",
      "969a3a1987bb4956bef188657c6b5ab4",
      "6693e47e0b7c4f0e826abc591dc96e72"
     ]
    },
    "id": "C1vkfb8TEVbC",
    "outputId": "3bd953db-e7c6-45e9-b492-e775f7e43629"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Load cleaned + labeled data\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "\n",
    "# Fill missing community labels if needed\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# Focus on top N communities\n",
    "TOP_N = 10\n",
    "top_communities = df[\"louvain_community\"].value_counts().head(TOP_N).index.tolist()\n",
    "df_top = df[df[\"louvain_community\"].isin(top_communities)].copy()\n",
    "\n",
    "# Get embeddings again (or reuse if stored)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(df_top[\"clean_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Run t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_results = tsne.fit_transform(embeddings)\n",
    "df_top[\"tsne-1\"] = tsne_results[:, 0]\n",
    "df_top[\"tsne-2\"] = tsne_results[:, 1]\n",
    "\n",
    "# Compute centroids for each community\n",
    "centroids = df_top.groupby(\"louvain_community\")[[\"tsne-1\", \"tsne-2\"]].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=\"tsne-1\", y=\"tsne-2\",\n",
    "    hue=\"louvain_community\",\n",
    "    palette=\"tab10\",\n",
    "    data=df_top,\n",
    "    s=30,\n",
    "    legend=\"full\"\n",
    ")\n",
    "\n",
    "# Add text labels at centroid positions\n",
    "for _, row in centroids.iterrows():\n",
    "    plt.text(\n",
    "        row[\"tsne-1\"], row[\"tsne-2\"],\n",
    "        f\"Community {int(row['louvain_community'])}\",\n",
    "        fontsize=12, weight='bold', color='black',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray', boxstyle='round,pad=0.3')\n",
    "    )\n",
    "\n",
    "plt.title(\"Louvain Communities (Top 10) ‚Äî t-SNE Layout\", fontsize=16)\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend(title=\"Community ID\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avt2WxzWEvE_"
   },
   "source": [
    "Descriptive Summary of Each Community\n",
    "python\n",
    "Copy\n",
    "Edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTcptsUzEv7q",
    "outputId": "97524c7a-88d9-4a1e-dd50-69a4aa2dd3bc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# Use only clean_text\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_top_words(texts, top_n=10):\n",
    "    words = \" \".join(texts).lower().split()\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    return Counter(words).most_common(top_n)\n",
    "\n",
    "# Generate summary per community\n",
    "summary = []\n",
    "\n",
    "for community_id in sorted(df[\"louvain_community\"].unique()):\n",
    "    subset = df[df[\"louvain_community\"] == community_id]\n",
    "    count = len(subset)\n",
    "    top_words = get_top_words(subset[\"clean_text\"].tolist())\n",
    "    sample_comment = subset[\"clean_text\"].iloc[0] if count > 0 else \"\"\n",
    "\n",
    "    summary.append({\n",
    "        \"community_id\": community_id,\n",
    "        \"comment_count\": count,\n",
    "        \"top_keywords\": [kw for kw, _ in top_words],\n",
    "        \"example_comment\": sample_comment\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df = summary_df.sort_values(\"comment_count\", ascending=False)\n",
    "\n",
    "# Display\n",
    "import pandas as pd\n",
    "print(summary_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmoxzQ5mFIb_"
   },
   "source": [
    " Summary for Top 10 Communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "id": "sjRbxlK4FLbE",
    "outputId": "874d55f3-32ee-40cb-dac5-40749ca43e0a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import textwrap\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# Focus only on Top 10 communities\n",
    "top_coms = df[\"louvain_community\"].value_counts().head(10).index.tolist()\n",
    "df_top = df[df[\"louvain_community\"].isin(top_coms)].copy()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_top_keywords(texts, n=5):\n",
    "    words = \" \".join(texts).lower().split()\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    return [kw for kw, _ in Counter(words).most_common(n)]\n",
    "\n",
    "# Wrap text to fixed width for display\n",
    "def clean_and_wrap(text, width=70):\n",
    "    return \"\\n\".join(textwrap.wrap(text.strip(), width=width))\n",
    "\n",
    "# Build summary\n",
    "summary = []\n",
    "for cid in top_coms:\n",
    "    subset = df_top[df_top[\"louvain_community\"] == cid]\n",
    "    count = len(subset)\n",
    "    keywords = get_top_keywords(subset[\"clean_text\"].tolist(), 5)\n",
    "    sample = clean_and_wrap(subset[\"clean_text\"].iloc[0])\n",
    "\n",
    "    summary.append({\n",
    "        \"Community ID\": cid,\n",
    "        \"Comment Count\": count,\n",
    "        \"Top Keywords\": \", \".join(keywords),\n",
    "        \"Example Comment\": sample\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "\n",
    "# Optional: style it for notebook display\n",
    "from IPython.display import display\n",
    "display(summary_df.style.set_properties(**{\n",
    "    'white-space': 'pre-wrap',\n",
    "    'text-align': 'left'\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3f1yB1VFdTA"
   },
   "source": [
    " t-SNE Plot with Keyword Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 875,
     "referenced_widgets": [
      "ef80046946474c82a397edbcf58fcf5b",
      "d9f940db5b5d4580b3b0bf408dc2eada",
      "0aeaf167cff048afae6fb7ba82ea1c18",
      "4182a76fc8214eef9c839d34026e1303",
      "258e937719614cedbf86e04883a65461",
      "17332bc1eb6e409eb7a62454093b5b63",
      "cc9ec632ed914eae89d4becd2b4c92fa",
      "739eca906e3340b491e2199328f131fb",
      "9f084cdf64f34604936e829b1f995655",
      "f093467794e54e6b9493e36ce7a72486",
      "edb84848ac2e44c0820107a7be9c707e"
     ]
    },
    "id": "eB3EtkPEFd2v",
    "outputId": "ae37dbf8-93ab-457b-c44c-4882b3a69af1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import nltk\n",
    "import textwrap\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# Focus on top 10 communities\n",
    "top_coms = df[\"louvain_community\"].value_counts().head(10).index.tolist()\n",
    "df_top = df[df[\"louvain_community\"].isin(top_coms)].copy()\n",
    "\n",
    "# Sentence embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(df_top[\"clean_text\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# t-SNE projection\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "tsne_result = tsne.fit_transform(embeddings)\n",
    "df_top[\"tsne-1\"] = tsne_result[:, 0]\n",
    "df_top[\"tsne-2\"] = tsne_result[:, 1]\n",
    "\n",
    "# Compute centroids\n",
    "centroids = df_top.groupby(\"louvain_community\")[[\"tsne-1\", \"tsne-2\"]].mean().reset_index()\n",
    "\n",
    "# Compute top keywords per community\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def get_top_keywords(texts, n=4):\n",
    "    words = \" \".join(texts).lower().split()\n",
    "    words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "    return [kw for kw, _ in Counter(words).most_common(n)]\n",
    "\n",
    "keyword_labels = {}\n",
    "for com in top_coms:\n",
    "    texts = df_top[df_top[\"louvain_community\"] == com][\"clean_text\"].tolist()\n",
    "    keywords = get_top_keywords(texts, 4)\n",
    "    label = \", \".join(keywords)\n",
    "    keyword_labels[com] = label\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=df_top,\n",
    "    x=\"tsne-1\", y=\"tsne-2\",\n",
    "    hue=\"louvain_community\",\n",
    "    palette=\"tab10\",\n",
    "    legend=False,\n",
    "    s=30\n",
    ")\n",
    "\n",
    "# Annotate with keyword labels\n",
    "for _, row in centroids.iterrows():\n",
    "    com = int(row[\"louvain_community\"])\n",
    "    label = keyword_labels.get(com, f\"Community {com}\")\n",
    "    plt.text(\n",
    "        row[\"tsne-1\"], row[\"tsne-2\"],\n",
    "        label,\n",
    "        fontsize=11,\n",
    "        weight=\"bold\",\n",
    "        bbox=dict(facecolor=\"white\", edgecolor=\"gray\", boxstyle=\"round,pad=0.3\", alpha=0.75)\n",
    "    )\n",
    "\n",
    "plt.title(\"Top Reddit Comment Communities (Louvain) ‚Äî t-SNE with Keyword Labels\", fontsize=15)\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA1RBT2dFzwM"
   },
   "source": [
    "Sentiment Analysis per Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395,
     "referenced_widgets": [
      "0639b76588ef4dcfa69554fc61a4a693",
      "40a209940b1e465aa6ff8a259c8809f6",
      "d24e631dfdf3470bbb63300028484f4a",
      "20aa36817a9e4d0482cc0d03440de61e",
      "ca548540352240dfb987ded0e139aa91",
      "0a1a62ca4426432d92157ad0f0f226fb",
      "11ce29c366c34d69ae23920625023e33",
      "5f4b8ff95564482f9068118c95cf14f0",
      "0c0380057c7842f4a298e8c07b946401",
      "94690483a43d4ae289243f95a7606a66",
      "6ed1d9846856438f97452c80b470983d",
      "96365a5e81fd480cbd1c8b11070b18a4",
      "e891943a863745a0aa75735738df4500",
      "a42f4cc4fbea4dde922fb5eeb0f64ead",
      "342944f1dd5a43b0bdfb0878975688c2",
      "019799376495406caa01140c7820d522",
      "d8737f7cab4a47aab1a012c4a2c7a91c",
      "518411cd76024528810c96e33e2244c3",
      "c075e65eb6b04747814262d123aab14d",
      "21206bcb442f4eea87bacdece07b96cd",
      "29f5d09190cf43c8b5c535cb371f3544",
      "a428e254ac504798be4d44e5a9f35c0b",
      "8a71684086264ad7a57cc9d6e7cad39d",
      "7390f24e94264fd09078e148674c93fe",
      "95c20059cbff40caab182508bb809aaf",
      "c8c1763748964f7dadcc88ff8cb1989f",
      "49acf2b44dd245a19c712302570cf4d6",
      "8832b9b3a6b04fc696d2daf1c7c5cc2a",
      "fad9f422c41546e98af377152ecc17e0",
      "860fa25d77d84c9fb358c382cf029564",
      "495258f45118459296dd1f2c2888e61d",
      "403c87b66e684f6d8b00db5b92a64220",
      "59f5f35755a54804abaaa7c67ea7b816",
      "9b72615068e24c5283aa53a7845f354e",
      "335990e59b004a1a8d6ea851760586ed",
      "158b1139fef84d03abdc5f450c699fdf",
      "dd67d0ef81104160bb7dfa3b046eabf6",
      "38e1edd746e24fea970c136b925871fa",
      "ae21a595cb1544aaa3e38180a6d01daa",
      "523c37f4b48440448c38053826d9cc93",
      "8d792a584d234304b61e2cfc989a0f50",
      "2c92581ff46d4f7b84599446184ef92d",
      "49d0b21718ef464797185fbaf4a4a973",
      "e81b73215e664e62a5d86b3e58a4cf1f"
     ]
    },
    "id": "8HwMSavQF1Zx",
    "outputId": "7e312fec-83f9-4a76-c456-bfec9cb71783"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "df_top[\"sentiment\"] = df_top[\"clean_text\"].apply(lambda x: sentiment_model(x[:512])[0]['label'])\n",
    "\n",
    "# Group by community\n",
    "sentiment_summary = df_top.groupby(\"louvain_community\")[\"sentiment\"].value_counts(normalize=True).unstack().fillna(0)\n",
    "print(sentiment_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jEw7nVI-GdX-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFCdGw2VGFW9"
   },
   "source": [
    "Evaluate how semantically tight or noisy each community is.\n",
    "\n",
    "Use intra-cluster cosine distances between embeddings\n",
    "\n",
    "Lower average distance ‚Üí tighter community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRyQIU83GFiF",
    "outputId": "3e896c6f-a909-49cb-a127-708e5a6ef56b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "compactness = []\n",
    "for com in top_coms:\n",
    "    subset = df_top[df_top[\"louvain_community\"] == com]\n",
    "    embs = model.encode(subset[\"clean_text\"].tolist())\n",
    "    avg_dist = cosine_distances(embs).mean()\n",
    "    compactness.append((com, avg_dist))\n",
    "\n",
    "compactness = sorted(compactness, key=lambda x: x[1])\n",
    "print(\"üìè Community Compactness (lower is better):\")\n",
    "for cid, dist in compactness:\n",
    "    print(f\"Community {cid}: {dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhRKeDrhGeZw"
   },
   "source": [
    "Bar Chart of Sentiment Distribution per Community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "qUav3iAkGelP",
    "outputId": "3a7aa3d7-ed26-43ba-fa4a-12c1444acc30"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reuse the sentiment_summary you created earlier\n",
    "sentiment_summary = df_top.groupby(\"louvain_community\")[\"sentiment\"].value_counts(normalize=True).unstack().fillna(0)\n",
    "\n",
    "# Plot bar chart\n",
    "sentiment_summary = sentiment_summary[[\"POSITIVE\", \"NEGATIVE\"]]  # consistent order\n",
    "\n",
    "sentiment_summary.plot(\n",
    "    kind=\"bar\",\n",
    "    stacked=True,\n",
    "    figsize=(10, 5),\n",
    "    colormap=\"coolwarm\"\n",
    ")\n",
    "\n",
    "plt.title(\"üìä Sentiment Distribution per Louvain Community\", fontsize=15)\n",
    "plt.ylabel(\"Proportion of Comments\")\n",
    "plt.xlabel(\"Louvain Community\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6eLYaGpGlM7"
   },
   "source": [
    "Add Sentiment as Hover Info in Interactive t-SNE Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "4t35HmuqGleo",
    "outputId": "e0106a56-6711-4c97-d382-bba7d2bcda1c"
   },
   "outputs": [],
   "source": [
    "!pip install plotly -q\n",
    "import plotly.express as px\n",
    "\n",
    "# Reduce long text for hover\n",
    "df_top[\"short_comment\"] = df_top[\"clean_text\"].apply(lambda x: x[:100] + \"...\" if len(x) > 100 else x)\n",
    "\n",
    "fig = px.scatter(\n",
    "    df_top,\n",
    "    x=\"tsne-1\", y=\"tsne-2\",\n",
    "    color=\"louvain_community\",\n",
    "    hover_data=[\"short_comment\", \"sentiment\"],\n",
    "    title=\"üåê t-SNE of Reddit Comments ‚Äî Hover Sentiment & Text\",\n",
    "    color_continuous_scale=\"Viridis\"\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "fig.update_layout(legend_title=\"Louvain Community\", height=600)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDyDBmORHEyU"
   },
   "source": [
    "Degree Distribution for Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753,
     "referenced_widgets": [
      "cc11b2b29ddb4d1caef1a8f7e3eb6d16",
      "0b06f9b81f8f4150a1f894616b148367",
      "1b355b2a3fea42fcbac8675eed23827b",
      "e27fe230bed74f7d81cc478a635484ef",
      "1c5f9deb1cb34cf4b55342793fbc61a3",
      "310d83b055da416b90ce5de2d8f6d169",
      "5b69fb0b6a694725ad9f8fb45e67a18e",
      "f8e9b915e1274135845e477a1269b8a4",
      "0761777ed7304d2a93e2214f8b7461af",
      "07399a94b90b4df4ab4fa3eec15bad84",
      "781dae80673447b3b137fa0d2ad53f64"
     ]
    },
    "id": "O0Hnjt5NHE-h",
    "outputId": "e2de1e78-b199-4477-fc62-a3b79898a5db"
   },
   "outputs": [],
   "source": [
    "# Install dependencies (if not done already)\n",
    "!pip install -q sentence-transformers scikit-learn networkx matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load cleaned dataset (use comments or titles)\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts = df[\"clean_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "# Step 2: Encode using SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Step 3: Compute cosine similarity matrix\n",
    "print(\"Computing similarity matrix...\")\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Step 4: Build graph from similarity threshold\n",
    "threshold = 0.6\n",
    "G = nx.Graph()\n",
    "\n",
    "print(\"Building graph edges from similarity matrix...\")\n",
    "for i in tqdm(range(len(texts))):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        if similarity_matrix[i][j] > threshold:\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "print(f\"‚úÖ Graph built with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Step 5: Compute and plot degree distribution\n",
    "degrees = [val for (node, val) in G.degree()]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(degrees, bins=40, kde=False, color='darkblue')\n",
    "plt.title(\"üìä Degree Distribution of Reddit Comments Graph\", fontsize=15)\n",
    "plt.xlabel(\"Degree (Number of Similar Comments)\")\n",
    "plt.ylabel(\"Number of Comments (Nodes)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg9nn0UNMrum"
   },
   "source": [
    "Top commenters by degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6eeed8c537cf4dd08afac630133bc5d9",
      "caa357f0b47a4f03b2cc6fcd4744be0a",
      "6a72f0e80de9438a9aad019805b384db",
      "7970b9fe59ac425896b23a1ad2450bdc",
      "76b1167da7e841349e4074f69976d37d",
      "3159284d301941d98a13d172a33191ba",
      "c08a5acd410d442b8f797208d289cb70",
      "85dbe2bf4755496aa16405d6f7a22897",
      "ad05a40168c9468db4fbe164624a9ced",
      "f02010f068ce40909ee09f19920184f8",
      "bb550d3341d44e70b58c9ee6b1698121"
     ]
    },
    "id": "OyGXZLNWMsHZ",
    "outputId": "b34eb57b-0091-49bf-fc0f-0faea095c3c4"
   },
   "outputs": [],
   "source": [
    "# Install needed packages (if not already)\n",
    "!pip install -q sentence-transformers scikit-learn networkx matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG: Set to 'comment_body' or 'submission_title' ===\n",
    "use_column = \"submission_title\"  # üîÑ Change to \"clean_text\" for comments\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts = df[use_column].dropna().astype(str).tolist()\n",
    "\n",
    "# Step 2: Embed using SentenceTransformer\n",
    "print(f\"üîç Encoding {len(texts)} texts from column: '{use_column}'\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Step 3: Build cosine similarity graph\n",
    "print(\"üîó Computing similarity matrix...\")\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "threshold = 0.6\n",
    "G = nx.Graph()\n",
    "\n",
    "print(\"‚öôÔ∏è Building graph...\")\n",
    "for i in tqdm(range(len(texts))):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        if similarity_matrix[i][j] > threshold:\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "print(f\"‚úÖ Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Step 4: Degree distribution\n",
    "degrees = [val for (node, val) in G.degree()]\n",
    "degree_series = pd.Series(degrees)\n",
    "top_hubs = degree_series.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Step 5: Plot degree distribution and highlight hubs\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(degree_series, bins=40, kde=False, color=\"steelblue\", label=\"All nodes\")\n",
    "\n",
    "for idx, val in top_hubs.items():\n",
    "    plt.axvline(val, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.text(val, 5, f\"Top #{idx}\", rotation=90, verticalalignment='bottom', fontsize=8, color='red')\n",
    "\n",
    "plt.title(f\"üìä Degree Distribution ‚Äî {use_column.replace('_', ' ').title()}\", fontsize=15)\n",
    "plt.xlabel(\"Node Degree (number of connections)\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print top hub text\n",
    "print(\"\\nüîù Top 5 Hub Nodes by Degree:\")\n",
    "for idx in top_hubs.index[:5]:\n",
    "    print(f\"\\nNode #{idx} (degree {top_hubs[idx]}):\\n{texts[idx][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "59811f7c7d614ced802bdbe51d5091f7",
      "76d75cfd7af7476bb983566bef0ddcc2",
      "2fa0012f04e04d239001f5ae33382c2d",
      "e3e1779d1f2d4db2adceac62d8cdaf0e",
      "fd37ad71476940b1b268bbb0abc8237a",
      "8f40d6ca0c4249b7af2a02e79d2533bb",
      "ef8b5c4b6419414ba6e223fe0e32f12e",
      "00006e86e6e34954ade56f19362c2ac0",
      "1985e9ed21614686916d0210a2c13e0b",
      "96488e3b91404e24956279e326084c09",
      "9cf66b55fa2044699faa0516f29ab0cd"
     ]
    },
    "id": "L5eE-e6zNBWa",
    "outputId": "9330bfd3-88e7-4161-96a3-66a20e7b151f"
   },
   "outputs": [],
   "source": [
    "# Install needed packages (if not already)\n",
    "!pip install -q sentence-transformers scikit-learn networkx matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIG: Set to 'comment_body' or 'submission_title' ===\n",
    "use_column = \"submission_title\"  # üîÑ Change to \"clean_text\" for comments\n",
    "\n",
    "# Step 1: Load dataset\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts = df[use_column].dropna().astype(str).tolist()\n",
    "\n",
    "# Step 2: Embed using SentenceTransformer\n",
    "print(f\"üîç Encoding {len(texts)} texts from column: '{use_column}'\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# Step 3: Build cosine similarity graph\n",
    "print(\"üîó Computing similarity matrix...\")\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "threshold = 0.6\n",
    "G = nx.Graph()\n",
    "\n",
    "print(\"‚öôÔ∏è Building graph...\")\n",
    "for i in tqdm(range(len(texts))):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        if similarity_matrix[i][j] > threshold:\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "print(f\"‚úÖ Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Step 4: Degree distribution\n",
    "degrees = [val for (node, val) in G.degree()]\n",
    "degree_series = pd.Series(degrees)\n",
    "top_hubs = degree_series.sort_values(ascending=False).head(10)\n",
    "\n",
    "# Step 5: Plot degree distribution and highlight hubs\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(degree_series, bins=40, kde=False, color=\"steelblue\", label=\"All nodes\")\n",
    "\n",
    "for idx, val in top_hubs.items():\n",
    "    plt.axvline(val, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.text(val, 5, f\"Top #{idx}\", rotation=90, verticalalignment='bottom', fontsize=8, color='red')\n",
    "\n",
    "plt.title(f\"üìä Degree Distribution ‚Äî {use_column.replace('_', ' ').title()}\", fontsize=15)\n",
    "plt.xlabel(\"Node Degree (number of connections)\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print top hub text\n",
    "print(\"\\nüîù Top 5 Hub Nodes by Degree:\")\n",
    "for idx in top_hubs.index[:5]:\n",
    "    print(f\"\\nNode #{idx} (degree {top_hubs[idx]}):\\n{texts[idx][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lfj6n99NOPu"
   },
   "source": [
    "Degree Distribution per Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682,
     "referenced_widgets": [
      "2990eb9a526e415094d6504d574dcf07",
      "ff41c005ebb140e8a63a670fed879008",
      "49184a7a40fc4b8a8a72e3ab577a55af",
      "5abcd7b1dcd84e75b9a783cfc5addd6c",
      "bd5dc17cf0c7446991e8d8b18a63a447",
      "dd4d703228a9491f9497ea82bcaa7285",
      "300a57d68f4d4d74ade58d6b536effe7",
      "b4cc589031394817ade6449b3a45b29e",
      "93f538de32cc422fb965205a56e4c312",
      "4a347cf6d81a4a11a9a91c8e512f8035",
      "40438401e73a45a3859abac4680c01c8"
     ]
    },
    "id": "XHNoljIXNQDU",
    "outputId": "e2172e4c-b0f1-4699-ea7c-b7c964699367"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use submission titles or comment text\n",
    "texts = df[\"submission_title\"].dropna().astype(str).tolist()  # ‚Üê or \"clean_text\" for comments\n",
    "\n",
    "# Embedding\n",
    "embeddings = model.encode(texts, show_progress_bar=True)\n",
    "sim_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Build graph\n",
    "G_doc = nx.Graph()\n",
    "threshold = 0.6\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    for j in range(i + 1, len(texts)):\n",
    "        if sim_matrix[i, j] > threshold:\n",
    "            G_doc.add_edge(i, j, weight=sim_matrix[i, j])\n",
    "\n",
    "# Degree distribution (documents)\n",
    "doc_degrees = [val for (_, val) in G_doc.degree()]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(doc_degrees, bins=40, kde=False, color='darkgreen')\n",
    "plt.title(\"üìÑ Degree Distribution per Document (Titles)\", fontsize=14)\n",
    "plt.xlabel(\"Number of Similar Documents\")\n",
    "plt.ylabel(\"Number of Nodes\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z74MfsswNyMw"
   },
   "source": [
    "Log-Log Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954,
     "referenced_widgets": [
      "7f77d11811a74d598dda5bf2e7e0a69d",
      "27019f390473422ab0ca962cd5650008",
      "23d3bb6a409546cfb76257faaaf07670",
      "669ba2cc97454be48414bbdf9de58baa",
      "a78a777dfbb54e3d82ff4da1ea4ad7f7",
      "66a3c3bcffb1426088ea438c87f45c47",
      "7ad14d404e3341c48c91029d06e14c56",
      "4571eceaaec74ea9ac41bd6cbd0bb3e8",
      "b69c018eb9a644aab4e853bbdcf67d43",
      "11a8cd5e83d040d3bb5fef3547b5cb37",
      "8488e156eee246a593b480712b47ebfa"
     ]
    },
    "id": "PrffrR0hNyWq",
    "outputId": "2fdbf2a4-6001-4b40-b1b0-65a9235b2906"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers scikit-learn networkx nltk matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# === STEP 1: Load and prepare data ===\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts_doc = df[\"clean_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "# === STEP 2: Document Similarity Graph ===\n",
    "print(\"üîç Encoding documents...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings_doc = model.encode(texts_doc, show_progress_bar=True)\n",
    "\n",
    "print(\"üîó Computing cosine similarity matrix...\")\n",
    "sim_matrix = cosine_similarity(embeddings_doc)\n",
    "threshold = 0.4  # Lowered threshold for richer connectivity\n",
    "\n",
    "G_doc = nx.Graph()\n",
    "for i in range(len(texts_doc)):\n",
    "    for j in range(i + 1, len(texts_doc)):\n",
    "        if sim_matrix[i][j] > threshold:\n",
    "            G_doc.add_edge(i, j, weight=sim_matrix[i][j])\n",
    "\n",
    "doc_degrees = [val for (_, val) in G_doc.degree()]\n",
    "print(f\"üìÑ Document graph: {G_doc.number_of_nodes()} nodes, {G_doc.number_of_edges()} edges\")\n",
    "\n",
    "# === STEP 3: Word Co-occurrence Graph ===\n",
    "print(\"üî† Building word co-occurrence graph...\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return [word for word in text.split() if word not in stopwords.words(\"english\") and len(word) > 2]\n",
    "\n",
    "tokenized_texts = df[\"clean_text\"].dropna().astype(str).apply(preprocess)\n",
    "tokenized_texts = tokenized_texts[tokenized_texts.apply(len) > 5]\n",
    "\n",
    "G_word = nx.Graph()\n",
    "for tokens in tokenized_texts:\n",
    "    for w1, w2 in itertools.combinations(set(tokens), 2):\n",
    "        if G_word.has_edge(w1, w2):\n",
    "            G_word[w1][w2][\"weight\"] += 1\n",
    "        else:\n",
    "            G_word.add_edge(w1, w2, weight=1)\n",
    "\n",
    "word_degrees = [val for (_, val) in G_word.degree()]\n",
    "print(f\"üî† Word graph: {G_word.number_of_nodes()} nodes, {G_word.number_of_edges()} edges\")\n",
    "\n",
    "# === STEP 4: Log-log degree distribution plotting ===\n",
    "def plot_loglog_cleaned(degrees, title):\n",
    "    degree_counts = Counter(degrees)\n",
    "    ks = np.array(sorted(degree_counts.keys()))\n",
    "    pk = np.array([v / sum(degree_counts.values()) for v in degree_counts.values()])\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(ks, pk, s=40, alpha=0.7)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"p(k)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === STEP 5: Final plots ===\n",
    "plot_loglog_cleaned(doc_degrees, \"degree distribution for documents (log-log)\")\n",
    "plot_loglog_cleaned(word_degrees, \"degree distribution for words (log-log)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nOPIPluP-kl"
   },
   "source": [
    "for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954,
     "referenced_widgets": [
      "f2e960b86f874de38174b1ed117e930f",
      "93ea515f3b284f6f9391025bce445dd2",
      "7ee0f9c0701b4291b695e4c18d9b203e",
      "01ea35a573a54581a83f9cea751ef926",
      "e4d1d1e03c504413a767e5f49331298b",
      "7bd4e339e7ad4b978fbbc6000432025d",
      "0555b921c62c4ea4b6a3e75f4bc7bc3f",
      "60e4dbf795ab4d9da8a3fc46c1a61b97",
      "0bc851ce87f3484fb957909a89a9a4e3",
      "1100aa9664234c3b85de0afb49f92f27",
      "5bcd0bc6e879476faa9d2f1667caee83"
     ]
    },
    "id": "jX8kINDOP_hx",
    "outputId": "d4d3a8bf-b788-491f-9241-fcc923e23e90"
   },
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "!pip install -q sentence-transformers scikit-learn networkx nltk matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# === STEP 1: Load and prepare data ===\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts_doc = df[\"submission_title\"].dropna().astype(str).tolist()\n",
    "\n",
    "# === STEP 2: Document Similarity Graph (TITLES) ===\n",
    "print(\"üîç Encoding submission titles...\")\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings_doc = model.encode(texts_doc, show_progress_bar=True)\n",
    "\n",
    "print(\"üîó Computing cosine similarity matrix...\")\n",
    "sim_matrix = cosine_similarity(embeddings_doc)\n",
    "threshold = 0.4  # Lower threshold to form more edges\n",
    "\n",
    "G_doc = nx.Graph()\n",
    "for i in range(len(texts_doc)):\n",
    "    for j in range(i + 1, len(texts_doc)):\n",
    "        if sim_matrix[i][j] > threshold:\n",
    "            G_doc.add_edge(i, j, weight=sim_matrix[i][j])\n",
    "\n",
    "doc_degrees = [val for (_, val) in G_doc.degree()]\n",
    "print(f\"üìÑ Title graph: {G_doc.number_of_nodes()} nodes, {G_doc.number_of_edges()} edges\")\n",
    "\n",
    "# === STEP 3: Word Co-occurrence Graph from Titles ===\n",
    "print(\"üî† Building word co-occurrence graph (titles)...\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return [word for word in text.split() if word not in stopwords.words(\"english\") and len(word) > 2]\n",
    "\n",
    "tokenized_titles = df[\"submission_title\"].dropna().astype(str).apply(preprocess)\n",
    "tokenized_titles = tokenized_titles[tokenized_titles.apply(len) > 3]\n",
    "\n",
    "G_word = nx.Graph()\n",
    "for tokens in tokenized_titles:\n",
    "    for w1, w2 in itertools.combinations(set(tokens), 2):\n",
    "        if G_word.has_edge(w1, w2):\n",
    "            G_word[w1][w2][\"weight\"] += 1\n",
    "        else:\n",
    "            G_word.add_edge(w1, w2, weight=1)\n",
    "\n",
    "word_degrees = [val for (_, val) in G_word.degree()]\n",
    "print(f\"üî† Word graph (from titles): {G_word.number_of_nodes()} nodes, {G_word.number_of_edges()} edges\")\n",
    "\n",
    "# === STEP 4: Plotting Function (Cleaned log-log)\n",
    "def plot_loglog_cleaned(degrees, title):\n",
    "    degree_counts = Counter(degrees)\n",
    "    ks = np.array(sorted(degree_counts.keys()))\n",
    "    pk = np.array([v / sum(degree_counts.values()) for v in degree_counts.values()])\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(ks, pk, s=40, alpha=0.7)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"p(k)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# === STEP 5: Final Plots ===\n",
    "plot_loglog_cleaned(doc_degrees, \"degree distribution for titles (log-log)\")\n",
    "plot_loglog_cleaned(word_degrees, \"degree distribution for title-words (log-log)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWFqh0pmQRe9"
   },
   "source": [
    "adding powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrX2pZJlQVmR",
    "outputId": "b53bcc3f-e4be-49a7-d136-c62ca1ea55d1"
   },
   "outputs": [],
   "source": [
    "pip install powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C8OjtXS6QRmh",
    "outputId": "506e8c1a-0224-40d2-d3fe-d6af85d270b4"
   },
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fit_powerlaw_and_plot(degrees, title):\n",
    "    print(f\"\\nüìä Fitting power-law for: {title}\")\n",
    "\n",
    "    # Remove 0-degree nodes (not part of power-law support)\n",
    "    degrees = [d for d in degrees if d > 0]\n",
    "\n",
    "    # Fit the power-law\n",
    "    fit = powerlaw.Fit(degrees, discrete=True, verbose=False)\n",
    "\n",
    "    # Print exponent and comparison stats\n",
    "    print(f\"Estimated power-law exponent Œ≥: {fit.alpha:.3f}\")\n",
    "    print(f\"Minimum degree (xmin): {fit.xmin}\")\n",
    "    R, p = fit.distribution_compare('power_law', 'exponential')\n",
    "    print(f\"Likelihood ratio test vs exponential: R = {R:.3f}, p = {p:.3f}\")\n",
    "\n",
    "    # Plot with fitted line\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    fit.plot_ccdf(color='blue', label='Empirical data')\n",
    "    fit.power_law.plot_ccdf(color='red', linestyle='--', label='Power-law fit')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"P(X ‚â• k)\")\n",
    "    plt.title(f\"{title} (CCDF + Power-law fit)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "fit_powerlaw_and_plot(doc_degrees, \"Title Graph Degree Distribution\")\n",
    "fit_powerlaw_and_plot(word_degrees, \"Title-Word Co-occurrence Degree Distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny76yRPKQuSO"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QW3plHbOQucY"
   },
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def fit_powerlaw_and_plot_clean(degrees, title):\n",
    "    degrees = [d for d in degrees if d > 0]  # Remove isolated nodes\n",
    "\n",
    "    fit = powerlaw.Fit(degrees, discrete=True, verbose=False)\n",
    "\n",
    "    print(f\"\\nüìä {title}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  ‚Ä¢ Nodes analyzed: {len(degrees)}\")\n",
    "    print(f\"  ‚Ä¢ Power-law exponent Œ≥: {fit.alpha:.3f}\")\n",
    "    print(f\"  ‚Ä¢ xmin (fit starts at degree ‚â•): {fit.xmin}\")\n",
    "    R, p = fit.distribution_compare('power_law', 'exponential')\n",
    "    print(f\"  ‚Ä¢ Power-law vs Exponential: R = {R:.2f}, p = {p:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Plot CCDF + fitted power-law\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    fit.plot_ccdf(label=\"Empirical CCDF\", color=\"black\", linewidth=2, marker='o', markersize=4, alpha=0.8)\n",
    "    fit.power_law.plot_ccdf(color=\"red\", linestyle=\"--\", label=\"Fitted Power-law\", linewidth=2)\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Degree k\", fontsize=12)\n",
    "    plt.ylabel(\"P(K ‚â• k)\", fontsize=12)\n",
    "    plt.title(f\"{title}\\n(Log-Log CCDF with Power-law Fit)\", fontsize=13)\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", lw=0.5, alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Be2nJ2oQxnd"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RkGuV4jWQyDH",
    "outputId": "4f8e0c5e-faae-4dd3-da9d-430c92345cef"
   },
   "outputs": [],
   "source": [
    "fit_powerlaw_and_plot_clean(doc_degrees, \"üìÑ Title Document Graph\")\n",
    "fit_powerlaw_and_plot_clean(word_degrees, \"üî† Title-Word Co-occurrence Graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U1KkhgbRDiJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def plot_clean_loglog(degrees, title=\"Degree Distribution (log-log)\", dot_color=\"black\"):\n",
    "    # Step 1: remove zeros\n",
    "    degrees = [d for d in degrees if d > 0]\n",
    "\n",
    "    # Step 2: get p(k)\n",
    "    degree_counts = Counter(degrees)\n",
    "    ks = np.array(sorted(degree_counts.keys()))\n",
    "    pk = np.array([v / sum(degree_counts.values()) for v in degree_counts.values()])\n",
    "\n",
    "    # Step 3: plot log-log\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(ks, pk, s=35, color=dot_color, alpha=0.8)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Degree k\", fontsize=12)\n",
    "    plt.ylabel(\"p(k)\", fontsize=12)\n",
    "    plt.title(title, fontsize=13)\n",
    "    plt.grid(True, which=\"both\", ls=\"--\", lw=0.5, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 940
    },
    "id": "T8wHX77oRGye",
    "outputId": "869c9f65-4dac-455d-ec92-d8b1302823f7"
   },
   "outputs": [],
   "source": [
    "plot_clean_loglog(doc_degrees, \"üìÑ Log-Log Degree Distribution for Titles\", dot_color=\"darkgreen\")\n",
    "plot_clean_loglog(word_degrees, \"üî† Log-Log Degree Distribution for Title Words\", dot_color=\"steelblue\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQH9NOdpRe8m",
    "outputId": "ee93a88b-b33c-42e6-9e08-af493dcb0308"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "\n",
    "print(\"üìã Columns in dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nüîç Sample rows:\")\n",
    "print(df[[\"submission_title\", \"comment_body\", \"clean_text\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUHuOYsdSYOK"
   },
   "source": [
    "Hard Louvain Cluster Size Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "mKhmpx42SYks",
    "outputId": "d6ec3246-5d60-4c10-c2cb-355f2511bddf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# Simulate soft Louvain if missing\n",
    "if \"soft_louvain\" not in df.columns:\n",
    "    df[\"soft_louvain\"] = df[\"louvain_community\"]\n",
    "\n",
    "# Normalize counts\n",
    "hard_counts = df[\"louvain_community\"].value_counts(normalize=True).sort_index()\n",
    "soft_counts = df[\"soft_louvain\"].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Filter to cluster IDs 0‚Äì20\n",
    "hard_counts = hard_counts[hard_counts.index <= 70]\n",
    "soft_counts = soft_counts[soft_counts.index <= 70]\n",
    "\n",
    "# === Plot side-by-side standard chart ===\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "# Hard Louvain\n",
    "axs[0].scatter(hard_counts.index, hard_counts.values, color='royalblue', s=50)\n",
    "axs[0].set_title(\"Hard Louvain\", fontsize=13)\n",
    "axs[0].set_xlabel(\"Community ID\")\n",
    "axs[0].set_ylabel(\"Proportion\")\n",
    "axs[0].set_xlim(-1.5, 40)\n",
    "axs[0].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Soft Louvain\n",
    "axs[1].scatter(soft_counts.index, soft_counts.values, color='cornflowerblue', s=50)\n",
    "axs[1].set_title(\"Soft Louvain\", fontsize=13)\n",
    "axs[1].set_xlabel(\"Community ID\")\n",
    "axs[1].set_xlim(-01.5, 70.5)\n",
    "axs[1].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Louvain Cluster Proportions (Standard View)\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAElf2TiUUQH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CMjPPZNUUbx",
    "outputId": "22fc08be-79e4-4f94-f948-59e96692222d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return [word for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# Limit to top 10 communities\n",
    "top_coms = df[\"louvain_community\"].value_counts().head(10).index.tolist()\n",
    "summary = []\n",
    "\n",
    "for com in top_coms:\n",
    "    subset = df[df[\"louvain_community\"] == com][\"clean_text\"].dropna().astype(str)\n",
    "    all_words = []\n",
    "    for text in subset:\n",
    "        all_words.extend(clean_and_tokenize(text))\n",
    "\n",
    "    top_words = Counter(all_words).most_common(10)\n",
    "    keywords = \", \".join([w for w, _ in top_words])\n",
    "\n",
    "    summary.append({\n",
    "        \"Community ID\": com,\n",
    "        \"Comment Count\": len(subset),\n",
    "        \"Top Keywords\": keywords\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for viewing\n",
    "keywords_df = pd.DataFrame(summary)\n",
    "keywords_df = keywords_df.sort_values(\"Community ID\")\n",
    "keywords_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Show table\n",
    "import pandas as pd\n",
    "# Show top 10 Louvain communities and their keywords\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "print(keywords_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v0X950WkUi1t",
    "outputId": "7f5e3101-4931-4795-ab72-9ded7a9f6db1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# === Helper functions ===\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return [word for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# === Select top 5 communities by size ===\n",
    "top_communities = df[\"louvain_community\"].value_counts().head(10).index.tolist()\n",
    "\n",
    "# === Plot top 10 words for each ===\n",
    "fig, axs = plt.subplots(len(top_communities), 1, figsize=(8, 4 * len(top_communities)))\n",
    "\n",
    "for i, com in enumerate(top_communities):\n",
    "    subset = df[df[\"louvain_community\"] == com][\"clean_text\"].dropna().astype(str)\n",
    "    all_words = []\n",
    "    for text in subset:\n",
    "        all_words.extend(clean_and_tokenize(text))\n",
    "\n",
    "    word_counts = Counter(all_words).most_common(10)\n",
    "    words, counts = zip(*word_counts)\n",
    "\n",
    "    axs[i].barh(words, counts, color='royalblue')\n",
    "    axs[i].invert_yaxis()\n",
    "    axs[i].set_title(f\"Louvain Community {com} - Top Words\", fontsize=13)\n",
    "    axs[i].set_xlabel(\"Frequency\")\n",
    "    axs[i].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jUuVQIfWVDht",
    "outputId": "3dd2009c-f11d-40fd-c9a3-2e1b140cc816"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.cm as cm\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# === Helper functions ===\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return [word for word in text.split() if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"reddit_with_louvain.csv\")\n",
    "df[\"louvain_community\"] = df[\"louvain_community\"].fillna(-1).astype(int)\n",
    "\n",
    "# === Select top N communities ===\n",
    "TOP_COMMUNITIES = 8\n",
    "TOP_WORDS = 15\n",
    "\n",
    "top_communities = df[\"louvain_community\"].value_counts().head(TOP_COMMUNITIES).index.tolist()\n",
    "colors = cm.get_cmap('tab10', TOP_COMMUNITIES)  # Use categorical colormap\n",
    "\n",
    "# === Plot ===\n",
    "fig, axs = plt.subplots(TOP_COMMUNITIES, 1, figsize=(10, 4 * TOP_COMMUNITIES))\n",
    "\n",
    "for i, com in enumerate(top_communities):\n",
    "    subset = df[df[\"louvain_community\"] == com][\"clean_text\"].dropna().astype(str)\n",
    "    all_words = []\n",
    "    for text in subset:\n",
    "        all_words.extend(clean_and_tokenize(text))\n",
    "\n",
    "    word_counts = Counter(all_words).most_common(TOP_WORDS)\n",
    "    words, counts = zip(*word_counts)\n",
    "\n",
    "    axs[i].barh(words, counts, color=colors(i))\n",
    "    axs[i].invert_yaxis()\n",
    "    axs[i].set_title(f\"üåê Louvain Community {com} ‚Äî Top {TOP_WORDS} Words\", fontsize=13)\n",
    "    axs[i].set_xlabel(\"Frequency\")\n",
    "    axs[i].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Top Words per Louvain Community\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5W0RghQV4-x"
   },
   "source": [
    "Fit BERTopic on Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "referenced_widgets": [
      "63e6580c8b5a437fb4b32f5aa3b79089",
      "60db5269dcc74460bfeb7c93bec5dc73",
      "ddb9b002a3c948f49be3655914d36a91",
      "4d5d103e9c364530a8061a34cc679265",
      "26d8865cd6544f9cbe745730666563eb",
      "cf16e03a411746aabee424049b63f616",
      "afe917c5f2774f0abf04e5c7bd338748",
      "2f19b0a6bcda4ec786361d073594975b",
      "0c35972f65b4496898394604b566b12f",
      "4b2baf2edacc4eb1ba3cce213d810c46",
      "597fa6e2c9bd4ec8ac9600a116f0ccca"
     ]
    },
    "id": "N0RIulgfV7hJ",
    "outputId": "406b1213-b8e0-413a-d913-30064bc2567e"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "!pip install -q bertopic sentence-transformers umap-learn hdbscan\n",
    "\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"reddit_climate_cleaned.csv\")\n",
    "texts = df[\"clean_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "# Define models\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\", min_df=10, max_df=0.8)\n",
    "\n",
    "# Fit BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(texts)\n",
    "\n",
    "# Assign topics to DataFrame\n",
    "df[\"bertopic_topic\"] = topics\n",
    "df.to_csv(\"reddit_with_bertopic.csv\", index=False)\n",
    "print(\"‚úÖ BERTopic finished and results saved to 'reddit_with_bertopic.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3Dj8DajWdeW"
   },
   "source": [
    "Visualizing Topic Size Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "0I-iFgBRWd6P",
    "outputId": "13e97b46-9cf1-423a-f56b-396418507ce9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load topic-labeled data\n",
    "df = pd.read_csv(\"reddit_with_bertopic.csv\")\n",
    "df[\"bertopic_topic\"] = df[\"bertopic_topic\"].fillna(-1).astype(int)\n",
    "\n",
    "# Compute proportions\n",
    "topic_counts = df[\"bertopic_topic\"].value_counts(normalize=True).sort_index()\n",
    "\n",
    "# Limit x-axis to first 20 topics\n",
    "topic_counts_plot = topic_counts[topic_counts.index <= 20]\n",
    "\n",
    "# Plot dot-style topic proportion chart\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(topic_counts_plot.index, topic_counts_plot.values, color=\"darkorange\", s=40)\n",
    "plt.title(\"BERTopic\", fontsize=14)\n",
    "plt.xlabel(\"Topic ID\")\n",
    "plt.ylabel(\"Proportion of Comments\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSWVWHsBWj_z"
   },
   "source": [
    "Log-Log CCDF of Topic Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "eeyoGM70Wkp8",
    "outputId": "82368b19-ce0c-4582-ed82-e1b908ecef53"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Get raw counts\n",
    "topic_degrees = df[\"bertopic_topic\"].value_counts().sort_index()\n",
    "degrees = np.array(topic_degrees.tolist())\n",
    "degrees = degrees[degrees > 0]\n",
    "\n",
    "# Compute p(k)\n",
    "count_vals = Counter(degrees)\n",
    "k = np.array(sorted(count_vals.keys()))\n",
    "pk = np.array([v / sum(count_vals.values()) for v in count_vals.values()])\n",
    "Pk = 1 - np.cumsum(pk)\n",
    "\n",
    "# Plot log-log CCDF\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(k, Pk, s=35, color=\"darkorange\", alpha=0.8)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"k (Topic Size)\")\n",
    "plt.ylabel(\"P(k ‚â• K)\")\n",
    "plt.title(\"BERTopic Topic Size CCDF (log-log)\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lHrfRrtsWvQt",
    "outputId": "66d8e87f-f728-4906-d338-d5a4a1a0aec3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Load topic-labeled data and model\n",
    "df = pd.read_csv(\"reddit_with_bertopic.csv\")\n",
    "\n",
    "# Re-initialize the BERTopic model if not in memory (optional: load saved model instead)\n",
    "# topic_model = BERTopic.load(\"bertopic_model\")  # If you've saved it\n",
    "# Otherwise, reuse from earlier in session\n",
    "\n",
    "# Count top N topics\n",
    "TOP_TOPICS = 8\n",
    "TOP_WORDS = 15\n",
    "\n",
    "top_topic_ids = df[\"bertopic_topic\"].value_counts().head(TOP_TOPICS).index.tolist()\n",
    "colors = cm.get_cmap('tab10', TOP_TOPICS)\n",
    "\n",
    "# Plot each topic's top words\n",
    "fig, axs = plt.subplots(TOP_TOPICS, 1, figsize=(10, 4 * TOP_TOPICS))\n",
    "\n",
    "for i, topic_id in enumerate(top_topic_ids):\n",
    "    words_scores = topic_model.get_topic(topic_id)[:TOP_WORDS]\n",
    "    words, scores = zip(*words_scores)\n",
    "\n",
    "    axs[i].barh(words, scores, color=colors(i))\n",
    "    axs[i].invert_yaxis()\n",
    "    axs[i].set_title(f\"Topic {topic_id} ‚Äî Top {TOP_WORDS} Words\", fontsize=13)\n",
    "    axs[i].set_xlabel(\"Score\")\n",
    "    axs[i].grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Top Keywords per BERTopic Topic\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUF5JWVzW7EW"
   },
   "source": [
    "BERTopic 2D UMAP Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639,
     "referenced_widgets": [
      "1251d87b343748f6a1c381d3c76f6c0d",
      "d1212d1d8fe04c77a090d54e4f4d389c",
      "d37568af5c164e7381effe8f7f67c4b7",
      "c83e3631808c4eb8a5b03061519e56b3",
      "c187fd6b2f614100a64c9e5693b9c47a",
      "799f7707f0304c5e81966f239fa8c19f",
      "2958d7ca38214238932a1fc4f5db9b01",
      "63c61c181534421a852109be0bc28d5c",
      "a8eed83e9114442cb7c8a28421c52c0e",
      "1b83118ee00f481f9c24ca1ae9f4916c",
      "b6da5fda04c748aa90427d452dc1f68b"
     ]
    },
    "id": "MeGLt-10W7hw",
    "outputId": "2f525a7b-ecab-484b-a0b9-72005ede1fdf"
   },
   "outputs": [],
   "source": [
    "# üì¶ Install required libraries (if not already done)\n",
    "!pip install -q bertopic sentence-transformers umap-learn hdbscan\n",
    "\n",
    "# üìÑ Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# üîÑ Load dataset\n",
    "df = pd.read_csv(\"reddit_with_bertopic.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "\n",
    "# üí¨ Sentence embeddings\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "# üß≠ Reduce embeddings to 2D using UMAP\n",
    "umap_model = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine')\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "df[\"x\"] = reduced_embeddings[:, 0]\n",
    "df[\"y\"] = reduced_embeddings[:, 1]\n",
    "\n",
    "# üîç Filter to top 10 topics only\n",
    "top_topic_ids = df[\"bertopic_topic\"].value_counts().head(10).index.tolist()\n",
    "df_filtered = df[df[\"bertopic_topic\"].isin(top_topic_ids)]\n",
    "\n",
    "# üé® Plot with Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_filtered,\n",
    "    x=\"x\", y=\"y\",\n",
    "    hue=\"bertopic_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=30,\n",
    "    alpha=0.85\n",
    ")\n",
    "plt.title(\"BERTopic UMAP Clusters (Top 10 Topics)\", fontsize=14)\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.legend(title=\"Topic\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "RRc02umHXvt3",
    "outputId": "fce3c6b8-c2ea-4610-dedb-9ab93ed1b749"
   },
   "outputs": [],
   "source": [
    "#Hoverable with top words per topic\n",
    "fig = topic_model.visualize_documents(texts, embeddings=embeddings)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 767
    },
    "id": "sc3jA3srYLis",
    "outputId": "5452a764-610b-447c-a6a8-626bec99a056"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load topic assignments\n",
    "df = pd.read_csv(\"reddit_with_bertopic.csv\")\n",
    "texts = df[\"clean_text\"].astype(str).tolist()\n",
    "topics = topic_model.get_document_info(texts)[\"Topic\"]\n",
    "\n",
    "# Get top 10 most frequent topics\n",
    "top_10_topic_ids = topics.value_counts().head(10).index.tolist()\n",
    "\n",
    "# Filter the texts and embeddings to those 10 topics\n",
    "selected_indices = [i for i, t in enumerate(topics) if t in top_10_topic_ids]\n",
    "texts_subset = [texts[i] for i in selected_indices]\n",
    "embeddings_subset = np.array([embeddings[i] for i in selected_indices])  # ‚úÖ FIXED HERE\n",
    "\n",
    "# Re-run interactive visualization on filtered data\n",
    "fig = topic_model.visualize_documents(texts_subset, embeddings=embeddings_subset)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yN2lUiExYmpg",
    "outputId": "5da691db-d1a4-4f17-9f2d-dc81764b223b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import cm\n",
    "\n",
    "TOP_N_TOPICS = 10\n",
    "TOP_N_WORDS = 15\n",
    "\n",
    "# Get top 10 topic IDs again\n",
    "top_topic_ids = df[\"bertopic_topic\"].value_counts().head(TOP_N_TOPICS).index.tolist()\n",
    "colors = cm.get_cmap('tab10', TOP_N_TOPICS)\n",
    "\n",
    "# Plot bar chart per topic\n",
    "fig, axs = plt.subplots(len(top_topic_ids), 1, figsize=(10, 4 * len(top_topic_ids)))\n",
    "\n",
    "for i, topic_id in enumerate(top_topic_ids):\n",
    "    topic_words = topic_model.get_topic(topic_id)[:TOP_N_WORDS]\n",
    "    if not topic_words: continue\n",
    "    words, scores = zip(*topic_words)\n",
    "\n",
    "    axs[i].barh(words, scores, color=colors(i))\n",
    "    axs[i].invert_yaxis()\n",
    "    axs[i].set_title(f\"Topic {topic_id} ‚Äî Top {TOP_N_WORDS} Words\", fontsize=13)\n",
    "    axs[i].set_xlabel(\"Relevance Score\")\n",
    "    axs[i].grid(True, linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Top Keywords per BERTopic Topic\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeebYl7GacpY"
   },
   "source": [
    "Topic Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "p_eiopZQaqy2",
    "outputId": "1799f1aa-4699-476e-b2ff-52d190a86a01"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Count and select top 20 topics\n",
    "topic_counts = df[\"bertopic_topic\"].value_counts().head(20).sort_values(ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=topic_counts.index.astype(str), y=topic_counts.values, palette=\"tab20\")\n",
    "plt.title(\"Top 20 BERTopic Topics by Comment Count\", fontsize=15)\n",
    "plt.xlabel(\"Topic ID\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tq6OkHUJa8jp"
   },
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Reduce embeddings to 2D\n",
    "umap_model = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine')\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "\n",
    "# Add UMAP coordinates to df\n",
    "df[\"x\"] = reduced_embeddings[:, 0]\n",
    "df[\"y\"] = reduced_embeddings[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "Y1eHGtp8bGit",
    "outputId": "a91d3ec0-8e6a-4320-88b9-e8567916a68e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "top_n = 10\n",
    "top_topics = df[\"bertopic_topic\"].value_counts().head(top_n).index.tolist()\n",
    "df_filtered = df[df[\"bertopic_topic\"].isin(top_topics)]\n",
    "\n",
    "# Compute centroids and top words\n",
    "centroids = df_filtered.groupby(\"bertopic_topic\")[[\"x\", \"y\"]].mean()\n",
    "top_words = {\n",
    "    topic: topic_model.get_topic(topic)[0][0] if topic_model.get_topic(topic) else f\"Topic {topic}\"\n",
    "    for topic in top_topics\n",
    "}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_filtered,\n",
    "    x=\"x\", y=\"y\",\n",
    "    hue=\"bertopic_topic\",\n",
    "    palette=\"tab10\",\n",
    "    s=25,\n",
    "    alpha=0.7,\n",
    "    legend=False\n",
    ")\n",
    "\n",
    "for topic, row in centroids.iterrows():\n",
    "    label = top_words[topic]\n",
    "    plt.text(\n",
    "        row[\"x\"], row[\"y\"],\n",
    "        label,\n",
    "        fontsize=10,\n",
    "        weight='bold',\n",
    "        bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray')\n",
    "    )\n",
    "\n",
    "plt.title(\"BERTopic Clusters with Top Keywords\", fontsize=14)\n",
    "plt.xlabel(\"UMAP-1\")\n",
    "plt.ylabel(\"UMAP-2\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBwLj4fhbaK4"
   },
   "source": [
    "Similarity Matrix for Comments and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyNbYAYQbENW",
    "outputId": "50dfe331-20bb-4f78-8d3d-7bb3b645090a"
   },
   "outputs": [],
   "source": [
    "TOP_N_TOPICS = 10\n",
    "TOP_N_WORDS = 10\n",
    "\n",
    "top_topic_ids = df[\"bertopic_topic\"].value_counts().head(TOP_N_TOPICS).index.tolist()\n",
    "\n",
    "for topic_id in top_topic_ids:\n",
    "    keywords = topic_model.get_topic(topic_id)\n",
    "    word_list = [word for word, score in keywords[:TOP_N_WORDS]]\n",
    "    print(f\"\\nüîπ Topic {topic_id}:\")\n",
    "    print(\"   \" + \", \".join(word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839,
     "referenced_widgets": [
      "3bf0344c5c714c24807ded905ea36243",
      "369e2e2c19e245cd99e22a6c0fcd1755",
      "3aca998e8296455fa596f2e151263da0",
      "a5cf66aa2dc24745afa1126cdbfec04a",
      "eb348fdf584b45f0b208a2d2b7365f8b",
      "6e365ea436034eeaa4df6a70c122a670",
      "ac33222ed9944c238610a3682cb48855",
      "5ec443d27d9e4dca95e19c17ab83450f",
      "e3fd96743769496b95664b451338818b",
      "bc9c322be21f402893cdfe7e04aea118",
      "051b470cb9ed42fdba1bacb58a416250"
     ]
    },
    "id": "JR2oNtQHdZm2",
    "outputId": "5d69f485-bd7d-4bee-be4c-2d01f23bdfb7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample 10 comments with topic labels\n",
    "sample_df = df[[\"comment_body\", \"bertopic_topic\"]].dropna().sample(10, random_state=1)\n",
    "sample_texts = sample_df[\"comment_body\"].astype(str).tolist()\n",
    "sample_labels = [f\"Topic {tid}\" for tid in sample_df[\"bertopic_topic\"]]\n",
    "\n",
    "# Encode\n",
    "sample_embeddings = embedding_model.encode(sample_texts, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "sim_matrix = cosine_similarity(sample_embeddings)\n",
    "\n",
    "# Plot heatmap with topic IDs as y-axis\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", cmap=\"YlOrRd\",\n",
    "            xticklabels=sample_labels, yticklabels=sample_labels)\n",
    "plt.title(\"Comment Similarity Matrix (10 samples) ‚Äî Labeled by Topic\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938,
     "referenced_widgets": [
      "e57901c89501439f81c997a8c101fb11",
      "725777caccbb4639aaf8f5533f1b4a64",
      "09024322de23474dbc740778237d2bfc",
      "dde95cef7cb540b68c3c9801e90eee5a",
      "441ca3ded208436b9ccbeceb1688e568",
      "1d59d9ad88164f02995d7ea4afe66913",
      "2da6536899174a20be41475d08ac0b14",
      "385a134f5cdb4767ae90fea17be00ae3",
      "fffd153cf294447a8bbc31e511a7304a",
      "bae691e6b9564c68b5da28fed516877b",
      "a982031625fc4c208919965d7a01f326"
     ]
    },
    "id": "t6N4eRa1fvY6",
    "outputId": "0788a6e9-d85d-4366-c6e7-2867aaf37e25"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Get top 3 keywords for a given topic\n",
    "def get_top_words(topic_id, n=3):\n",
    "    try:\n",
    "        words = topic_model.get_topic(topic_id)\n",
    "        if words and len(words) >= n:\n",
    "            return \", \".join([w for w, _ in words[:n]])\n",
    "        elif words:\n",
    "            return \", \".join([w for w, _ in words])\n",
    "        else:\n",
    "            return \"unknown topic\"\n",
    "    except:\n",
    "        return \"unknown topic\"\n",
    "\n",
    "# Step 2: Collect 10 comments from 10 different valid topics\n",
    "selected_comments = []\n",
    "selected_labels = []\n",
    "used_topics = set()\n",
    "\n",
    "# Loop through dataframe and pick 1 comment per unique, valid topic\n",
    "for _, row in df.dropna(subset=[\"comment_body\", \"bertopic_topic\"]).iterrows():\n",
    "    topic_id = int(row[\"bertopic_topic\"])\n",
    "    if topic_id not in used_topics:\n",
    "        label = get_top_words(topic_id)\n",
    "        if label != \"unknown topic\":\n",
    "            selected_comments.append(str(row[\"comment_body\"]))\n",
    "            selected_labels.append(label)\n",
    "            used_topics.add(topic_id)\n",
    "    if len(selected_comments) == 10:\n",
    "        break\n",
    "\n",
    "# Step 3: Generate embeddings and similarity\n",
    "comment_embeddings = embedding_model.encode(selected_comments, show_progress_bar=True)\n",
    "sim_matrix = cosine_similarity(comment_embeddings)\n",
    "\n",
    "# Step 4: Plot the similarity matrix\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n",
    "            xticklabels=selected_labels, yticklabels=selected_labels)\n",
    "plt.title(\"Comment Similarity Matrix ‚Äî Top 3 Words per 10 Unique Topics\", fontsize=15)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8stSJkt0irWa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938,
     "referenced_widgets": [
      "73424444e4c341439eaaacefa8826c48",
      "b33ed897fd2b4692990d9a05c01f11db",
      "0ca8f619e1ca4c26a5a73408a2781135",
      "214a56c8921a468bb22420bc9fd8323f",
      "1cae435acde243d282420ddcf6b3289f",
      "fa01cc5109334881a4583ca9dbf097ea",
      "72d3dd832c4c4a24b2bd76fda5043cd2",
      "25ff521f52d242b485764d3bcefb48ad",
      "0df52f5eeb9a475fabd3a283d13efa8e",
      "ba0fc09da0fc41b3ae1e311110a9ae88",
      "c9b82357ab7c4b42b73a229ec7ee6754"
     ]
    },
    "id": "T5BFZFFtirj6",
    "outputId": "fab7af7d-8623-4fb3-9e0b-d1a535dfb4e0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Get top 10 most frequent BERTopic topics\n",
    "top_topics = df[\"bertopic_topic\"].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Step 2: Get 1 representative title from each of these topics\n",
    "selected_titles = []\n",
    "selected_labels = []\n",
    "\n",
    "def get_top_words(topic_id, n=3):\n",
    "    try:\n",
    "        words = topic_model.get_topic(topic_id)\n",
    "        if words:\n",
    "            return \", \".join([w for w, _ in words[:n]])\n",
    "        else:\n",
    "            return f\"Topic {topic_id}\"\n",
    "    except:\n",
    "        return f\"Topic {topic_id}\"\n",
    "\n",
    "for topic_id in top_topics:\n",
    "    title_row = df[(df[\"bertopic_topic\"] == topic_id) & (df[\"submission_title\"].notna())]\n",
    "    if not title_row.empty:\n",
    "        title = title_row.iloc[0][\"submission_title\"]\n",
    "        selected_titles.append(str(title))\n",
    "        selected_labels.append(get_top_words(topic_id))\n",
    "\n",
    "# Step 3: Generate embeddings and similarity matrix\n",
    "title_embeddings = embedding_model.encode(selected_titles, show_progress_bar=True)\n",
    "sim_matrix = cosine_similarity(title_embeddings)\n",
    "\n",
    "# Step 4: Plot heatmap\n",
    "plt.figure(figsize=(12, 9))\n",
    "sns.heatmap(sim_matrix, annot=True, fmt=\".2f\", cmap=\"Oranges\",\n",
    "            xticklabels=selected_labels, yticklabels=selected_labels)\n",
    "plt.title(\"Submission Title Similarity ‚Äî Top 10 Most Frequent Topics\", fontsize=15)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "02097ebe7dba4327a490b3d6e26926bb",
      "08b8a87b10524336943ee2c30856d936",
      "6c681fd0fe4540cf9e7108ff35f17057",
      "2a73eb970a7a4c9a88b43293d8cb5e48",
      "713d82cd57604478af8557df36897d06",
      "955e7b6ca4784a5eb1515378c955c18a",
      "09bd643313fa47a3b756e40541c1e19b",
      "44e3c222fe9c4e26984d43872b610d3d",
      "7874bc235ef14842a6f723ccd503c123",
      "5b864f0783fb4c27afc9009bc83be0b2",
      "a63b88b48e6c402dbcfa030a8ee51972"
     ]
    },
    "id": "dRgOHoNTjYka",
    "outputId": "835c565b-9c72-455b-b257-6236c73fd077"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# üî¢ Step 1: Get top 20 most frequent topics\n",
    "TOP_N = 20\n",
    "top_topics = df[\"bertopic_topic\"].value_counts().head(TOP_N).index.tolist()\n",
    "\n",
    "# üß† Helper: Get top 3 topic words\n",
    "def get_top_words(topic_id, n=3):\n",
    "    try:\n",
    "        words = topic_model.get_topic(topic_id)\n",
    "        if words:\n",
    "            return \", \".join([w for w, _ in words[:n]])\n",
    "        else:\n",
    "            return f\"Topic {topic_id}\"\n",
    "    except:\n",
    "        return f\"Topic {topic_id}\"\n",
    "\n",
    "# üîç Step 2: Select 1 representative title per topic\n",
    "selected_titles = []\n",
    "selected_labels = []\n",
    "\n",
    "for topic_id in top_topics:\n",
    "    title_row = df[(df[\"bertopic_topic\"] == topic_id) & (df[\"submission_title\"].notna())]\n",
    "    if not title_row.empty:\n",
    "        title = title_row.iloc[0][\"submission_title\"]\n",
    "        selected_titles.append(str(title))\n",
    "        selected_labels.append(get_top_words(topic_id))\n",
    "\n",
    "# üß† Step 3: Compute embeddings + cosine similarity\n",
    "title_embeddings = embedding_model.encode(selected_titles, show_progress_bar=True)\n",
    "sim_matrix = cosine_similarity(title_embeddings)\n",
    "\n",
    "# üìä Step 4: Plot heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(sim_matrix, annot=False, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "            xticklabels=selected_labels, yticklabels=selected_labels)\n",
    "plt.title(\"Submission Title Similarity ‚Äî Top 20 Topics (Top 3 Keywords Each)\", fontsize=16)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egHghrIxjqbr"
   },
   "source": [
    " Hierarchical Clustering on Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 801,
     "referenced_widgets": [
      "1b261c6ec0e84292beaf9a8a252bb52e",
      "87291007493244ffbf365d39481d62a2",
      "9ebd647fd4b44ed280d5d29a1e18d9ad",
      "48d2bd652346415eb8300fd80667c89c",
      "318e5d203da241bbbca9f6136a6371ba",
      "29af69559b32430ea2f18daa87ceac42",
      "85ace25d5b6c43dcaede5e15643ce91d",
      "4ced13490c5a417695120804688e2aa7",
      "9b039c48b346465b93efff41997d2d08",
      "c3bc88f3ccc74d50b2a4deba82ddbf48",
      "e15bc25b17bd4c42b9779b468f72fdcc"
     ]
    },
    "id": "XYmo_U9wjs5M",
    "outputId": "b5338391-7ea3-4805-cdf6-b83fe15a3a2c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Get top N topics\n",
    "TOP_N = 20\n",
    "top_topics = df[\"bertopic_topic\"].value_counts().head(TOP_N).index.tolist()\n",
    "\n",
    "# Step 2: Select 1 comment per topic\n",
    "selected_comments = []\n",
    "labels = []\n",
    "\n",
    "def get_top_words(topic_id, n=3):\n",
    "    topic = topic_model.get_topic(topic_id)\n",
    "    if topic:\n",
    "        return \", \".join([w for w, _ in topic[:n]])\n",
    "    return f\"Topic {topic_id}\"\n",
    "\n",
    "for topic_id in top_topics:\n",
    "    row = df[(df[\"bertopic_topic\"] == topic_id) & (df[\"comment_body\"].notna())]\n",
    "    if not row.empty:\n",
    "        comment = row.iloc[0][\"comment_body\"]\n",
    "        selected_comments.append(str(comment))\n",
    "        labels.append(get_top_words(topic_id))\n",
    "\n",
    "# Step 3: Embed comments and compute cosine distance\n",
    "comment_embeddings = embedding_model.encode(selected_comments, show_progress_bar=True)\n",
    "distance_matrix = 1 - cosine_similarity(comment_embeddings)\n",
    "\n",
    "# Step 4: Apply hierarchical clustering\n",
    "linked = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Step 5: Plot dendrogram\n",
    "plt.figure(figsize=(14, 8))\n",
    "dendrogram(linked, labels=labels, leaf_rotation=45, leaf_font_size=11)\n",
    "plt.title(\"Hierarchical Clustering of Comments (Top 20 Topics)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pySKVQphj2XS"
   },
   "source": [
    "for titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f99SHn9zj36Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 803,
     "referenced_widgets": [
      "87e9e4bd7f604c69878c55394190bc19",
      "f292ce2bea2449f590dfac43ab0c2427",
      "0e00634ca9e44b8ab824960a487f1826",
      "cd76067857c047afa57261f1d6d3acdf",
      "6aa61c8d65fb4d7ba67567f286597185",
      "6ee99c4fa33b44ff963cdbcef33b7347",
      "abb9e2a80fc04929ae15230f58d6b830",
      "e1a5e30a24ad48d6ba70733585b2bf03",
      "0ef037938a2d45b297e29846d8bf66ef",
      "85034a50d9264672a1656805d22f72e7",
      "996202beb7ec4f9bb14860f4911e855d"
     ]
    },
    "id": "i3yl90PQj38I",
    "outputId": "509bdaba-2c5d-4db9-d613-179240c91b67"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Get top N topics\n",
    "TOP_N = 20\n",
    "top_topics = df[\"bertopic_topic\"].value_counts().head(TOP_N).index.tolist()\n",
    "\n",
    "# Step 2: Select 1 title per topic\n",
    "selected_titles = []\n",
    "labels = []\n",
    "\n",
    "def get_top_words(topic_id, n=3):\n",
    "    topic = topic_model.get_topic(topic_id)\n",
    "    if topic:\n",
    "        return \", \".join([w for w, _ in topic[:n]])\n",
    "    return f\"Topic {topic_id}\"\n",
    "\n",
    "for topic_id in top_topics:\n",
    "    row = df[(df[\"bertopic_topic\"] == topic_id) & (df[\"submission_title\"].notna())]\n",
    "    if not row.empty:\n",
    "        title = row.iloc[0][\"submission_title\"]\n",
    "        selected_titles.append(str(title))\n",
    "        labels.append(get_top_words(topic_id))\n",
    "\n",
    "# Step 3: Embed titles and compute cosine distance\n",
    "title_embeddings = embedding_model.encode(selected_titles, show_progress_bar=True)\n",
    "distance_matrix = 1 - cosine_similarity(title_embeddings)\n",
    "\n",
    "# Step 4: Apply hierarchical clustering\n",
    "linked = linkage(distance_matrix, method='ward')\n",
    "\n",
    "# Step 5: Plot dendrogram\n",
    "plt.figure(figsize=(14, 8))\n",
    "dendrogram(linked, labels=labels, leaf_rotation=45, leaf_font_size=11)\n",
    "plt.title(\"Hierarchical Clustering of Titles (Top 20 Topics)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfU3GCV-kUNX"
   },
   "source": [
    "world clouds for top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q-zsdgYskX3_",
    "outputId": "09c74850-4488-46c5-ebe5-7cbf93f45dc5"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Get top 10 most frequent topics\n",
    "top_topics = df[\"bertopic_topic\"].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Step 2: Generate word clouds\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, topic_id in enumerate(top_topics):\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "\n",
    "    if topic_words:\n",
    "        # Convert to frequency dict {word: weight}\n",
    "        word_freq = {word: score for word, score in topic_words}\n",
    "\n",
    "        # Generate word cloud\n",
    "        wc = WordCloud(width=800, height=400, background_color='white', colormap='Set2')\n",
    "        wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "        # Plot\n",
    "        plt.subplot(5, 2, i + 1)\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Topic {topic_id}\", fontsize=14)\n",
    "\n",
    "plt.suptitle(\"BERTopic ‚Äî Word Clouds of Top 10 Topics\", fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wb3ZI7srkmd0"
   },
   "source": [
    "Word Clouds for Top 10 Louvain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "AxRzts07koQ7",
    "outputId": "4bc9c305-3bbb-4799-e5a9-16ab063a45f6"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Filter to rows with valid Louvain label\n",
    "valid_df = df[df[\"louvain_community\"].notna()].copy()\n",
    "\n",
    "# Get top 10 communities\n",
    "top_communities = valid_df[\"louvain_community\"].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Preprocess function\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, community_id in enumerate(top_communities):\n",
    "    community_texts = valid_df[valid_df[\"louvain_community\"] == community_id][\"comment_body\"].dropna()\n",
    "\n",
    "    # Tokenize and count words\n",
    "    all_tokens = []\n",
    "    for text in community_texts:\n",
    "        all_tokens.extend(clean_and_tokenize(str(text)))\n",
    "    word_freq = dict(Counter(all_tokens).most_common(100))\n",
    "\n",
    "    # Generate word cloud\n",
    "    wc = WordCloud(width=800, height=400, background_color='white', colormap='Set3')\n",
    "    wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "    # Plot\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Louvain Community {int(community_id)}\", fontsize=14)\n",
    "\n",
    "plt.suptitle(\"Word Clouds for Top 10 Louvain Communities\", fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "eac2b42cc4df48269a96ba5e98a974d7",
      "44338f5661a94b06899afcf782f20e65",
      "2d250802333d4e18818607410bbc71f5",
      "57f4f41bc6f54810b248e13ddbf05a82",
      "af09d60c8b8b42578a705cf315d8786b",
      "57f61626987044478fe0cd0f0fe44c9f",
      "0772e49e4fc64d38ba54665d93392944",
      "1d8466117b824cc5be8b7940c01a9dbf",
      "0622579070614896bc6290a728992349",
      "9bfa722d71c1412cbd8790552ff2dc24",
      "ae0c241de7b24b96b233a38845b79f07"
     ]
    },
    "id": "JC75tl1YkxZH",
    "outputId": "9b1b5f48-bb1f-415f-edc0-556edbe5fae3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from community import best_partition\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Subset titles (e.g., top 500 non-empty ones)\n",
    "title_df = df[df[\"submission_title\"].notna()].copy().reset_index(drop=True)\n",
    "title_df = title_df.head(500)  # You can increase to 1000 if GPU allows\n",
    "\n",
    "# Step 2: Compute embeddings\n",
    "titles = title_df[\"submission_title\"].astype(str).tolist()\n",
    "title_embeddings = embedding_model.encode(titles, show_progress_bar=True)\n",
    "\n",
    "# Step 3: Compute cosine similarity\n",
    "similarity_matrix = cosine_similarity(title_embeddings)\n",
    "\n",
    "# Step 4: Build similarity graph with threshold\n",
    "threshold = 0.6\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i + 1, len(similarity_matrix)):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        if sim > threshold:\n",
    "            G.add_edge(i, j, weight=sim)\n",
    "\n",
    "# Step 5: Run Louvain\n",
    "partition = best_partition(G)\n",
    "\n",
    "# Step 6: Save results\n",
    "title_df[\"louvain_title_community\"] = title_df.index.map(partition)\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Clean + tokenize function\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Get top 10 title communities\n",
    "top_comms = title_df[\"louvain_title_community\"].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Plot settings\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "for i, comm in enumerate(top_comms):\n",
    "    texts = title_df[title_df[\"louvain_title_community\"] == comm][\"submission_title\"].dropna()\n",
    "    all_tokens = []\n",
    "    for txt in texts:\n",
    "        all_tokens.extend(clean_and_tokenize(str(txt)))\n",
    "    word_freq = dict(Counter(all_tokens).most_common(100))\n",
    "\n",
    "    wc = WordCloud(width=800, height=400, background_color='white', colormap='tab10')\n",
    "    wc.generate_from_frequencies(word_freq)\n",
    "\n",
    "    plt.subplot(5, 2, i + 1)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Louvain Title Community {comm}\", fontsize=14)\n",
    "\n",
    "plt.suptitle(\"Word Clouds for Top 10 Louvain Title Communities\", fontsize=18)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0FglZJpHn5Nw",
    "outputId": "8b5c0238-c8bf-43e9-dc59-00291dec8541"
   },
   "outputs": [],
   "source": [
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Step 1: Clean subset (remove NaN community)\n",
    "subset = title_df.dropna(subset=[\"louvain_title_community\"]).copy()\n",
    "\n",
    "# Step 2: Filter to top 6 largest communities for clarity\n",
    "top_comms = subset[\"louvain_title_community\"].value_counts().head(6).index.tolist()\n",
    "subset = subset[subset[\"louvain_title_community\"].isin(top_comms)]\n",
    "\n",
    "# Step 3: Reduce embeddings with UMAP\n",
    "title_embeddings_subset = title_embeddings[subset.index]\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\")\n",
    "umap_coords = reducer.fit_transform(title_embeddings_subset)\n",
    "subset[\"x\"] = umap_coords[:, 0]\n",
    "subset[\"y\"] = umap_coords[:, 1]\n",
    "\n",
    "# Step 4: Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    data=subset,\n",
    "    x=\"x\", y=\"y\",\n",
    "    hue=\"louvain_title_community\",\n",
    "    palette=\"tab10\",\n",
    "    alpha=0.85,\n",
    "    s=90,\n",
    "    edgecolor='white',\n",
    "    linewidth=0.5\n",
    ")\n",
    "plt.title(\"UMAP Projection of Title Embeddings (Top 6 Louvain Communities)\", fontsize=16)\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.legend(title=\"Community\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    tokens = text.split()\n",
    "    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]\n",
    "    return tokens\n",
    "\n",
    "# Filter to top 6 Louvain title communities\n",
    "top_comms = title_df[\"louvain_title_community\"].value_counts().head(6).index.tolist()\n",
    "subset = title_df[title_df[\"louvain_title_community\"].isin(top_comms)]\n",
    "\n",
    "# Extract top 10 words per community\n",
    "summary = []\n",
    "for comm in top_comms:\n",
    "    texts = subset[subset[\"louvain_title_community\"] == comm][\"submission_title\"].dropna().astype(str)\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        all_tokens.extend(clean_and_tokenize(text))\n",
    "    top_words = [word for word, _ in Counter(all_tokens).most_common(10)]\n",
    "    summary.append({\"Community\": comm, \"Top Words\": \", \".join(top_words)})\n",
    "\n",
    "# Create and display table\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuASBJYyo8js"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
